{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoing Price Predictor\n",
    "\n",
    "\n",
    "We want to predict the price of bitcoin using historical bitcoin data as well as its sentiment analysis over time. We are planning on experimenting with time series models such as LSTM to predict token price. To measure sentiment analysis, we will be using the fear and greed API which measures engagement with Bitcoin. \n",
    "\n",
    "Tutorial: https://www.relataly.com/stock-market-prediction-using-multivariate-time-series-in-python/1815/#h-implementing-a-multivariate-time-series-prediction-model-in-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries to be used: \n",
    "import math # Mathematical functions\n",
    "import numpy as np # Fundamental package for scientific computing with Python\n",
    "import pandas as pd # Additional functions for analysing and manipulating data\n",
    "from datetime import date, timedelta, datetime # Date Functions\n",
    "from pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates\n",
    "import matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data\n",
    "import matplotlib.dates as mdates # Formatting dates\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors\n",
    "from keras.models import Sequential # Deep learning library, used for neural networks\n",
    "from keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers\n",
    "from keras.callbacks import EarlyStopping # EarlyStopping during model training\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merge_bitcoin_n_fear.csv\")\n",
    "# df = pd.read_csv(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert fear and greed labels to integers\n",
    "def label_fng_classification(row):\n",
    "    if row['fng_classification'] == 'Extreme Fear':\n",
    "        return 0\n",
    "    elif row['fng_classification'] == 'Fear':\n",
    "        return 1\n",
    "    elif row['fng_classification'] == 'Neutral':\n",
    "        return 2\n",
    "    elif row['fng_classification'] == 'Greed':\n",
    "        return 3\n",
    "    elif row['fng_classification'] == 'Extreme Greed':\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing labels to something we can work with\n",
    "df['fng_class'] = df.apply(lambda row: label_fng_classification(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing batches \n",
    "for minute to minute dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing Batches\n",
    "#minut to minute df\n",
    "# df = df.dropna()\n",
    "# print(df.size) #(3613769, 7)\n",
    "# df = df[len(df)//2:]\n",
    "# print(df.size) #(1806885, 7)\n",
    "# df.set_index('Timestamp', inplace=True)\n",
    "# train_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ogranizing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organizing csv\n",
    "df = df.dropna()\n",
    "df['Date'] = pd.to_datetime(df.Date, format = '%m-%d-%Y')\n",
    "train_df = df.sort_values(by=['Date']).copy()\n",
    "date_index = train_df.index\n",
    "train_df = train_df.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of considered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = ['Open','High','Low','Close','Volume_(BTC)','Volume_(Currency)','Weighted_Price'] #only market data\n",
    "FEATURES = ['Open','High','Low','Close','Volume_(BTC)','Volume_(Currency)','Weighted_Price','fng_value','fng_class']\n",
    "# FEATURES = ['Open','High','Low','Close','Volume_(BTC)','Volume_(Currency)','Weighted_Price','fng_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset with features and filter the data to the list of FEATURES\n",
    "data = pd.DataFrame(train_df)\n",
    "data_filtered = data[FEATURES]\n",
    "print(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add a prediction column and set dummy values to prepare the data for scaling\n",
    "data_filtered_ext = data_filtered.copy()\n",
    "data_filtered_ext['Prediction'] = data_filtered_ext['Weighted_Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows in the data\n",
    "nrows = data_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to numpy values\n",
    "np_data_unscaled = np.array(data_filtered)\n",
    "np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
    "print(np_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data by scaling each feature to a range between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "np_data_scaled = scaler.fit_transform(np_data_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate scaler that works on a single column for scaling predictions\n",
    "scaler_pred = MinMaxScaler()\n",
    "df_Close = pd.DataFrame(data_filtered_ext['Weighted_Price'])\n",
    "np_Close_scaled = scaler_pred.fit_transform(df_Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sequence length - this is the timeframe used to make a single prediction\n",
    "sequence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Index\n",
    "# index_Close = data.columns.get_loc(\"Weighted_Price\")\n",
    "index_Close = data_filtered.columns.get_loc(\"Weighted_Price\")\n",
    "print(index_Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data into train and train data sets\n",
    "As a first step, we get the number of rows to train the model on 80% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test data\n",
    "train_data = np_data_scaled[0:train_data_len, :]\n",
    "test_data = np_data_scaled[train_data_len - sequence_length:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RNN needs data with the format of [samples, time steps, features]\n",
    "# Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
    "def partition_dataset(sequence_length, data):\n",
    "    x, y = [], []\n",
    "    data_len = data.shape[0]\n",
    "    for i in range(sequence_length, data_len):\n",
    "        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
    "        y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
    "    \n",
    "    # Convert the x and y to numpy arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data and test data\n",
    "x_train, y_train = partition_dataset(sequence_length, train_data)\n",
    "x_test, y_test = partition_dataset(sequence_length, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the prediction value and the input match up\n",
    "# The last close price of the second input sample should equal the first prediction value\n",
    "# print(x_train[1][sequence_length-1][index_Close])\n",
    "# print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the neural network model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\n",
    "n_neurons = x_train.shape[1] * x_train.shape[2]\n",
    "# print(n_neurons, x_train.shape[1], x_train.shape[2])\n",
    "model.add(LSTM(n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(LSTM(n_neurons, return_sequences=True))\n",
    "model.add(LSTM(n_neurons, return_sequences=True))\n",
    "model.add(LSTM(n_neurons, return_sequences=True))\n",
    "model.add(LSTM(n_neurons, return_sequences=True))\n",
    "model.add(LSTM(n_neurons, return_sequences=True))\n",
    "\n",
    "# model.add(LSTM(n_neurons, return_sequences=True))\n",
    "# model.add(LSTM(n_neurons, return_sequences=True))\n",
    "# model.add(LSTM(n_neurons, return_sequences=True))\n",
    "model.add(LSTM(n_neurons, return_sequences=False))\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                  #   validation_data=(x_test, y_test)\n",
    "                  # )\n",
    "                    validation_data=(x_test, y_test),\n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training & validation losee values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "#fig, ax = plt.subplots(figsize=(20, 10), sharex=True)\n",
    "#plt.plot(history.history[\"loss\"])\n",
    "#plt.title(\"Model loss\")\n",
    "#plt.ylabel(\"Loss\")\n",
    "#plt.xlabel(\"Epoch\")\n",
    "#ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
    "#plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "#plt.grid()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "y_pred_scaled = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscale the predicted values\n",
    "y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
    "print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
    "print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # The date from which on the date is displayed\n",
    "# # display_start_date = pd.Timestamp('today') - timedelta(days=500)\n",
    "\n",
    "# # Add the date column\n",
    "# print(data_filtered)\n",
    "data_filtered_sub = data_filtered.copy()\n",
    "data_filtered_sub['Date'] = date_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the difference between the valid and predicted prices\n",
    "train = data_filtered_sub[:train_data_len + 1]\n",
    "valid = data_filtered_sub[train_data_len:]\n",
    "valid.insert(1, \"Prediction\", y_pred.ravel(), True)\n",
    "print(valid)\n",
    "print(valid[\"Weighted_Price\"])\n",
    "valid.insert(1, \"Difference\", valid[\"Prediction\"] - valid[\"Weighted_Price\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Zoom in to a closer timeframe\n",
    "# # valid = valid[valid['Date'] > display_start_date]\n",
    "# # train = train[train['Date'] > display_start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, ax1 = plt.subplots(figsize=(22, 10), sharex=True)\n",
    "xt = train['Date']; yt = train[[\"Weighted_Price\"]]\n",
    "xv = valid['Date']; yv = valid[[\"Weighted_Price\", \"Prediction\"]]\n",
    "plt.title(\"Predictions vs Actual Values\", fontsize=20)\n",
    "plt.ylabel(\"price in usd\", fontsize=18)\n",
    "plt.plot(xt, yt, color=\"#039dfc\", linewidth=2.0)\n",
    "plt.plot(xv, yv[\"Prediction\"], color=\"#E91D9E\", linewidth=2.0)\n",
    "plt.plot(xv, yv[\"Weighted_Price\"], color=\"black\", linewidth=2.0)\n",
    "plt.legend([\"Train\", \"Test Predictions\", \"Actual Values\"], loc=\"upper left\")\n",
    "plt.savefig(\"Predictions_vs_Actual.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the bar plot with the differences\n",
    "x = valid['Date']\n",
    "y = valid[\"Difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom color range for positive and negative differences\n",
    "valid.loc[y >= 0, 'diff_color'] = \"#2BC97A\"\n",
    "valid.loc[y < 0, 'diff_color'] = \"#C92B2B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x, y, width=0.8, color=valid['diff_color'])\n",
    "plt.grid()\n",
    "plt.savefig(\"Predictions_vs_Actual.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = df[-sequence_length:]\n",
    "# new_df = df_temp.filter(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = sequence_length\n",
    "\n",
    "# # Get the last N day closing price values and scale the data to be values between 0 and 1\n",
    "# last_N_days = new_df[-sequence_length:].values\n",
    "# last_N_days_scaled = scaler.transform(last_N_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list and Append past N days\n",
    "# X_test_new = []\n",
    "# X_test_new.append(last_N_days_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the X_test data set to a numpy array and reshape the data\n",
    "# pred_price_scaled = model.predict(np.array(X_test_new))\n",
    "# pred_price_unscaled = scaler_pred.inverse_transform(pred_price_scaled.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print last price and predicted price for the next day\n",
    "# predicted_price = np.round(pred_price_unscaled.ravel()[0], 2)\n",
    "# print(new_df['Weighted_Price'])\n",
    "# price_today = np.round(new_df['Weighted_Price'][-1], 2)\n",
    "# change_percent = np.round(100 - (price_today * 100)/predicted_price, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plus = '+'; minus = ''\n",
    "# print(f'The close price for Cake today was {price_today}')\n",
    "# print(f'The predicted close price is {predicted_price} ({plus if change_percent > 0 else minus}{change_percent}%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b346b40471ba213f5e961a5d782728ceb56ae92186b2dfa6f5b328dd50df76d4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
